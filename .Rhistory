fname <- paste(directory_path, "y_train.txt" ,sep="/")
trn_lable <- read.table(fname)
fname <- paste(directory_path, "subject_train.txt" ,sep="/")
trn_subj <- read.table(fname)
trn_set <- cbind(trn_set,trn_lable,trn_subj)
}
#load the data from the files into memory
run_analysis <- function(directory_path){
#**********reading traning dataset*******************
fname <- paste(directory_path, "train/X_train.txt" ,sep="/")
trn_set <- read.table(fname)
fname <- paste(directory_path, "train/y_train.txt" ,sep="/")
trn_lable <- read.table(fname)
fname <- paste(directory_path, "train/subject_train.txt" ,sep="/")
trn_subj <- read.table(fname)
trn_set <- cbind(trn_set,trn_lable,trn_subj)
}
run_analysis("C:/Users/mo/Documents/UCIDataset")
run_analysis("C:/Users/mo/Documents/UCIDataset")
run_analysis <- function(directory_path){
#**********reading traning dataset*******************
fname <- paste(directory_path, "train/X_train.txt" ,sep="/")
trn_set <- read.table(fname)
fname <- paste(directory_path, "train/y_train.txt" ,sep="/")
trn_lable <- read.table(fname)
fname <- paste(directory_path, "train/subject_train.txt" ,sep="/")
trn_subj <- read.table(fname)
trn_set <- cbind(trn_set,trn_lable,trn_subj)
trn_set
}
run_analysis("C:/Users/mo/Documents/UCIDataset")
features <- read.table("./UCIDataset/features.txt",header=FALSE,colClasses="character")
View(features)
View(features)
x <- data.frame(Category=factor(c("First", "First", "First", "Second",
"Third", "Third", "Second")),
Frequency=c(10,15,5,2,14,20,3))
aggregate(x$Frequency, by=list(Category=x$Category), FUN=sum)
?aggregate
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
dat
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
library(data.table)
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
source('~/run_Analysis.R')
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
View(x)
View(dat)
View(dat)
source('~/run_Analysis.R')
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
dat
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
source('~/run_Analysis.R')
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
View(dat)
View(dat)
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
View(dat)
View(dat)
source('~/run_Analysis.R')
source('~/run_Analysis.R')
dat <- run_Analysis("C:/Users/mo/Documents/UCIDataset")
dat
dat <- read.table("C:/Users/mo/Documents/UCIDataset/tidy_set.txt")
dat
View(dat)
View(dat)
source('~/run_Analysis.R')
source('~/run_Analysis.R')
? set.seed()
?seeds
z <- qnorm(.05)
mn <- 12
s <- 4
mu0 <- mn - z * s / sqrt(100)
mu0
qnorm(.05, 12, 4/10)
tt=qt(p=.025, df=8)
1100-tt*30
t.test(c(1,0,0,0), c(0, 1,1,1), alternative="greater", paired=T)
-4/sqrt(1.5^2/3+1.8^2/3)
pnorm(2.95, lower.tail=F)
pnorm(0.1/(.04/10)+qnorm(.95))
(qnorm(.95) + qnorm(.9)) ^ 2 * .04 ^ 2 / .01^2
pnorm(42.04, mean=44, sqrt(12/288+12/288))
t.test(c(1,0,0,0), c(0, 1,1,1), alternative="greater", paired=T)
pbinom(3,size=4,prob=0.5,lower.tail=FALSE)
pbinom(3,size=4,prob=0.75,lower.tail=FALSE)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
mean(x)
x <- x - 0.0025
x
x*x
x <- x*x
w*x
sum(w * x)
s <- (w * x)
mean(s)
mean(w)
s <-(w-1.7)
s
s * x
mean (w * x)
x <- c(0.18, -1.54, 0.42, 0.95)
mean(x)
(x-0.0025)^2
s <- (x-0.0025)^2
mean(s)
s <- x * w
mean(s)
sum(s-0.257)
s - 0.257)
s - 0.257
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
mean(w * x)
0.2575/4
s <- w * x
sum(s)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
sum(x*y)/sum(x*x)
install.packages("knitr")
install.packages("car")
data(mcar)
data(mtcars)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
options(RCurlOptions = list(verbose = FALSE, capath = system.file("CurlSSL", "cacert.pem", package = "RCurl"), ssl.verifypeer = FALSE))
library(caret)
install.packages("caret")
library(caret)
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
summary(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.9)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.9)
preProc$rotation
library(caret)
data(vowel.train)
data(vowel.test)
set.seed(33833)
vowel.train$y <-  factor(vowel.train$y)
vowel.test$y <-  factor(vowel.test$y)
set.seed(62433)
rfFit <- train( vowel.train$y ~ ., method="rf", data=vowel.train)
gbmFit <- train( vowel.train$y ~ ., method="gbm", data=vowel.train)
gdmPred <- predict(gbmFit,vowel.test)
rfPred <- predict(rfFit,vowel.test)
confusionMatrix(vowel.test$y,rfPred)
#  Accuracy : 0.5974
confusionMatrix(vowel.test$y,gdmPred)
# Accuracy : 0.5325
predDF <- data.frame(gdmPred,rfPred,y=vowel.test$y)
combModFit <- train(y ~.,method="gam", data=predDF)
combPred <- predict(combModFit,predDF)
confusionMatrix(vowel.test$y,combPred)
library(caret)
data(vowel.train)
data(vowel.test)
set.seed(33833)
vowel.train$y <-  factor(vowel.train$y)
vowel.test$y <-  factor(vowel.test$y)
library(caret)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
vowel.train$y <-  factor(vowel.train$y)
vowel.test$y <-  factor(vowel.test$y)
set.seed(62433)
rfFit <- train( vowel.train$y ~ ., method="rf", data=vowel.train)
gbmFit <- train( vowel.train$y ~ ., method="gbm", data=vowel.train)
gdmPred <- predict(gbmFit,vowel.test)
rfPred <- predict(rfFit,vowel.test)
confusionMatrix(vowel.test$y,rfPred)
#  Accuracy : 0.5974
confusionMatrix(vowel.test$y,gdmPred)
# Accuracy : 0.5325
predDF <- data.frame(gdmPred,rfPred,y=vowel.test$y)
combModFit <- train(y ~.,method="gam", data=predDF)
combPred <- predict(combModFit,predDF)
confusionMatrix(vowel.test$y,combPred)
library(caret)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
vowel.train$y <-  factor(vowel.train$y)
vowel.test$y <-  factor(vowel.test$y)
rfFit <- train( vowel.train$y ~ ., method="rf", data=vowel.train)
gbmFit <- train( vowel.train$y ~ ., method="gbm", data=vowel.train)
gdmPred <- predict(gbmFit,vowel.test)
rfPred <- predict(rfFit,vowel.test)
confusionMatrix(vowel.test$y,rfPred)
#  Accuracy : 0.5974
confusionMatrix(vowel.test$y,gdmPred)
# Accuracy : 0.5325
predDF <- data.frame(gdmPred,rfPred,y=vowel.test$y)
combModFit <- train(y ~.,method="gam", data=predDF)
combPred <- predict(combModFit,predDF)
confusionMatrix(vowel.test$y,combPred)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
rfFit <- train( training$diagnosis ~ ., method="rf", data=training)
btFit <- train( training$diagnosis ~ ., method="gbm", data=training)
ldaFit <- train( training$diagnosis ~ ., method="lda", data=training)
rfPred <- predict(rfFit,testing)
btPred <- predict(btFit,testing)
ldaPred <- predict(ldaFit,testing)
library(ggplot2)
qplot(rfPred,btPred,coulour=diagnosis, data=testing)
predDF <- data.frame(rfPred,btPred,ldaPred,diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~., method="rf",data=predDF)
comPred <- predict(combModFit,predDF)
sqrt(sum( (as.numeric(rfPred)-as.numeric(testing$diagnosis))^2))
sqrt(sum( (as.numeric(btPred)-as.numeric(testing$diagnosis))^2))
sqrt(sum( (as.numeric(ldaPred)-as.numeric(testing$diagnosis))^2))
sqrt(sum( (as.numeric(comPred)-as.numeric(testing$diagnosis))^2))
confusionMatrix(testing$diagnosis,rfPred)
confusionMatrix(testing$diagnosis,btPred)
confusionMatrix(testing$diagnosis,ldaPred)
confusionMatrix(testing$diagnosis,comPred)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
lassoFit <- train( training$CompressiveStrength ~ ., method="lasso", data=training)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
lassoFit <- train( training$CompressiveStrength ~ ., method="lasso", data=training)
lassoPred <- predict(lassoFit,testing)
plot.enet(lassoFit$finalModel, xvar="penalty", use.color=T)
install.packages("twitteR")
install.packages(c("tm", "wordcloud"))
source('~/TwitterSearch.R')
SearchWBG
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
SearchWBG()
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
source('~/TwitterSearch.R')
SearchWBG()
source('~/TwitterSearch.R')
SearchWBG()
install.packages("rmr2")
library(car)
data(Hartnagel)
Hartnagel
install.packages("shiny")
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
runapp()
runApp()
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
runApp()
setwd('C:\Users\mo\Documents\GitHub\DataProdcut_Proj')
setwd(''~\\GitHub\\DataProdcut_Proj')
setwd(''~/GitHub/DataProdcut_Proj')
setwd('/GitHub/DataProdcut_Proj')
getwd()
setwd('C:/Users/mo/Documents/GitHub/DataProdcut_Proj')
runApp()
runApp()
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
runApp()
data(co2)
tail (co2)
co2
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='leney', token='C4BDCBC5108E968E37C0DCAB2D441C36', secret='wFYo5f2AAQ/BCx9H/2XRJdNpV+ySLsrsopc4M8hI')
devtools::install_github('rstudio/rscrypt')
runApp
runApp()
cdata <- read.dta("http://www.ats.ucla.edu/stat/data/crime.dta")
state.x77
summary(state.x77)
install.packages("polspline")
data(state)
state.pm <- polymars(state.region, state.x77, knots = 15, classify = TRUE, gcv = 1)
library(polspline)
state.pm <- polymars(state.region, state.x77, knots = 15, classify = TRUE, gcv = 1)
table(predict(state.pm, x = state.x77, classify = TRUE), state.region)
runApp()
co2
stackloss
bigmodel
iris
require(MASS)
data(Boston)
Boston
runApp()
car
mtcars
data(galton)
abline(lm(sheight ~ fheight, data=father.son),lty=1,lwd=2)
data(father.son)
abline(lm(sheight ~ fheight, data=father.son),lty=1,lwd=2)
father.son summary
summary(father.son)
summary(galton)
data(galton)
data(galton)
library(UsingR)
require("UsingR")
data(galton)
library(UsingR)
install.packages("UsingR")
install.packages("UsingR")
data(galton)
library(UsingR)
data(galton)
data(galton)
summary(galton)
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
summary(galton)
head(galton,10)
summary(galton)
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/ui.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
runApp()
source('~/GitHub/DataProdcut_Proj/server.R')
runApp()
runApp()
source('~/GitHub/DataProdcut_Proj/server.R')
runApp()
source('~/GitHub/DataProdcut_Proj/server.R')
runApp()
lmModel <- lm(child ~ parent, data=galton)
lmModel$coeff[1] + lmModel$coeff[2] * 80
childHeight <- lmModel$coeff[1] + lmModel$coeff[2] * 80
plot(galton$parent, lmModel$residuals, pch=19, col=”blue”)
summary(galton)
lmModel
lmModel$coeff[1]
childHeight <- function(parentHeight) {
lmModel$coeff[1] + lmModel$coeff[2] * parentHeight
}
childHeight(90)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
summary(galton)
source('~/GitHub/DataProdcut_Proj/server.R')
runApp()
runApp()
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
source('~/GitHub/DataProdcut_Proj/server.R')
runApp()
runApp()
source('~/GitHub/DataProdcut_Proj/server.R')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
source('~/GitHub/DataProdcut_Proj/server.R')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
